{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook processes OCR content contained in .json files, generated in Microsoft AI Studio and manually added to the 'source' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and set variables and directories.\n",
    "import os, sys, re, json, re, logging\n",
    "\n",
    "# Manually set the root directory path\n",
    "root_dir = '../'\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "from modules.exclusions import excluded_strings, excluded_patterns\n",
    "\n",
    "# Define the primary source file you're working with\n",
    "hansard_source_file_name = '3.August29-September25-1901'  \n",
    "# Define the hathi_starting_page. This is the unique Hathi identifier for the Google pdf, and the page in that pdf that corresponds to the first page of Hansard in the original digitized source. It is used to concatenate the url to the Hathi Trust source in the last cell of the notebook.\n",
    "hathi_id = 'uc1.32106019788253'\n",
    "hathi_starting_page = 21  \n",
    "\n",
    "# Concatenate hansard_source_file to source_dir and output_dir_path\n",
    "source_dir_path = os.path.join('../source', hansard_source_file_name)\n",
    "output_pages_dir_path = os.path.join('../output', 'pages', hansard_source_file_name)\n",
    "output_sessions_dir_path = os.path.join('../output', 'sessions', hansard_source_file_name)\n",
    "\n",
    "# Check if source directory exists\n",
    "if not os.path.exists(source_dir_path):\n",
    "    os.makedirs(source_dir_path)\n",
    "\n",
    "# Check if output directory exists\n",
    "if not os.path.exists(output_pages_dir_path):\n",
    "    os.makedirs(output_pages_dir_path)\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "logs_dir_path = os.path.join('../logs')\n",
    "if not os.path.exists(logs_dir_path):\n",
    "    os.makedirs(logs_dir_path)\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# If the logger has handlers, remove them\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "# Create a new file handler that overwrites the log file each time\n",
    "log_file_path = os.path.join(logs_dir_path, '3.August29-September25-1901_exclusions.log')\n",
    "file_handler = logging.FileHandler(log_file_path, mode='w')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create a logging format\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handler to the logger\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Log the excluded strings and patterns\n",
    "logger.info(f\"Excluded strings: {excluded_strings}, Excluded patterns: {excluded_patterns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the OCR content (manually produced in and downloaded from Microsoft AI Studio) and associated positional data to generate page files from the two column layout, removing digitization metadata.\n",
    "\n",
    "# Get all files in the source directory\n",
    "files_in_directory = os.listdir(source_dir_path)\n",
    "\n",
    "# Filter for json files\n",
    "json_files = [file for file in files_in_directory if file.endswith('.json')]\n",
    "\n",
    "# Now you can iterate over the json_files list and process each file\n",
    "for json_file in json_files:\n",
    "    input_source_file = os.path.join(source_dir_path, json_file)\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open(input_source_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Access the \"analyzeResult\"\n",
    "    analyze_result = data['analyzeResult']\n",
    "\n",
    "    # Iterate over each page\n",
    "    for page in analyze_result['pages']:\n",
    "        # Create a variable to store the page content\n",
    "        page_content = \"\"\n",
    "\n",
    "        # Calculate the x-coordinate that separates the two columns\n",
    "        column_separator = page['width'] / 2\n",
    "\n",
    "        # Divide the text lines into two groups based on their x-coordinate\n",
    "        left_column_lines = [line for line in page['lines'] if min(line['polygon'][::2]) < column_separator]\n",
    "        right_column_lines = [line for line in page['lines'] if min(line['polygon'][::2]) >= column_separator]\n",
    "\n",
    "        # Sort each group of text lines by their y-coordinate (top to bottom)\n",
    "        left_column_lines.sort(key=lambda line: min(line['polygon'][1::2]))\n",
    "        right_column_lines.sort(key=lambda line: min(line['polygon'][1::2]))\n",
    "\n",
    "        # Combine the sorted text lines from the two columns\n",
    "        sorted_lines = left_column_lines + right_column_lines\n",
    "\n",
    "\n",
    "        # Define the areas of the page you want to exclude\n",
    "        x_range = (0, 0.1 * page['width'])  # Capture the left ~10% of the page. Callibrate to suit.\n",
    "        y_range = (0.95 * page['height'], page['height'])  # Capture the bottom ~10% of the page. Callibrate to suit.\n",
    "        header_y_range = (0, 0.1 * page['height'])  # Capture the top ~10% of the page. Callibrate to suit.\n",
    "\n",
    "        # Iterate over each text line in the page\n",
    "        for line in sorted_lines:\n",
    "            # Check if any of the y-coordinates of the line fall within the y_range\n",
    "            y_coordinates = line['polygon'][1::2]\n",
    "            if any(y_range[0] <= y <= y_range[1] for y in y_coordinates) or any(header_y_range[0] <= y <= header_y_range[1] for y in y_coordinates):\n",
    "                # If y-coordinate condition is met, log the line and continue to next iteration\n",
    "                logging.info(f\"Excluded due to y_range or header_y_range: {line}\")\n",
    "                continue\n",
    "\n",
    "            # If condition is false, add the line to the page content\n",
    "            content = line['content']\n",
    "            page_content += content + \" \"\n",
    "\n",
    "        # Write the page content to a new file in the output directory\n",
    "        output_file_path = os.path.join(output_pages_dir_path, f'page_{page[\"pageNumber\"]}.md')\n",
    "        with open(output_file_path, 'w') as f:\n",
    "            f.write(page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the pages into a single master content file, for cleaning.\n",
    "\n",
    "combined_file = os.path.join(output_pages_dir_path, 'all_pages_master.txt')\n",
    "\n",
    "def sort_key(filename):\n",
    "    match = re.search(r'page_(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return float('inf')\n",
    "\n",
    "with open(combined_file, 'w') as outfile:\n",
    "    for filename in sorted(os.listdir(output_pages_dir_path), key=sort_key):\n",
    "        if filename.startswith('page_') and filename.endswith('.md'):\n",
    "            page_number = filename.replace('page_', '').replace('.md', '')\n",
    "            outfile.write(f'<page:{page_number}>\\n')\n",
    "            with open(os.path.join(output_pages_dir_path, filename), 'r') as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line.lstrip())  # strip leading whitespace\n",
    "                outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the master content file, using local processing\n",
    "\n",
    "# Read the content\n",
    "with open(combined_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Remove patterns matching excluded patterns\n",
    "for pattern in excluded_patterns:\n",
    "    lines = [re.sub(pattern, '', line) for line in lines]\n",
    "\n",
    "# Remove strings matching excluded strings\n",
    "for string in excluded_strings:\n",
    "    logging.info(f\"Removing string: {string}\")\n",
    "    lines = [line.replace(string, '') for line in lines]\n",
    "\n",
    "# Remove unwanted indentation\n",
    "lines = [line.lstrip() for line in lines]\n",
    "\n",
    "# Write the cleaned content back\n",
    "master_file_path = os.path.join(output_pages_dir_path, 'all_pages_master.txt')\n",
    "with open(master_file_path, 'w') as file:\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell includes new_pattern to capture an instance specific to this volume, and account for poor ocr.\n",
    "\n",
    "# Write the cleaned content back\n",
    "master_file_path = os.path.join(output_pages_dir_path, 'all_pages_master.txt')\n",
    "with open(master_file_path, 'w') as file:\n",
    "    file.writelines(lines)\n",
    "\n",
    "# Open the master file for reading\n",
    "with open(master_file_path, 'r') as file:\n",
    "    # Read the content\n",
    "    content = file.read()\n",
    "\n",
    "    # Define the pattern for 'LEGISLATIVE COUNCIL.' followed by a date\n",
    "    pattern = r'(LEGISLATIVE COUNCIL\\.\\s*(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday), (?:\\d{1,2}|[IVX]{1,3})(?:st|nd|rd|th) \\w+, \\d{4})'\n",
    "\n",
    "    # Define the pattern for the specific string 'Thursday, 29th August, 1901.'\n",
    "    # Enhanced to account for potential OCR errors\n",
    "    hor_pattern = r'Thursday,\\s*29th\\s*August,\\s*1901'\n",
    "\n",
    "    # Combine both patterns\n",
    "    combined_pattern = f'{pattern}|{hor_pattern}'\n",
    "\n",
    "    # Find all matches of the combined pattern\n",
    "    matches = re.findall(combined_pattern, content)\n",
    "\n",
    "    # Check if hor_pattern is found\n",
    "    hor_matches = re.findall(hor_pattern, content)\n",
    "\n",
    "    # Split the content based on the combined pattern\n",
    "    parts = re.split(combined_pattern, content)\n",
    "\n",
    "    # Iterate over the parts, skipping the first\n",
    "    for i in range(1, len(parts), 2):\n",
    "        # Ensure the part is a string\n",
    "        if not isinstance(parts[i], str):\n",
    "            continue\n",
    "\n",
    "        # Extract the date string following 'LEGISLATIVE COUNCIL.' or 'Thursday, 29th August, 1901.'\n",
    "        date_match = re.search(r'(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday), (?:\\d{1,2}|[IVX]{1,3})(?:st|nd|rd|th) \\w+, \\d{4}|Thursday,\\s*29th\\s*August,\\s*1901', parts[i])\n",
    "        date = date_match.group() if date_match else 'No date'\n",
    "\n",
    "        # Format the filename\n",
    "        filename = f'{i//2+1}-{date}.txt'\n",
    "\n",
    "        # Concatenate output_dir_path with subdirectory and filename\n",
    "        file_path = os.path.join(output_sessions_dir_path, filename)\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "        # Check if the part starts with a <page> tag\n",
    "        if not parts[i].strip().startswith('<page>'):\n",
    "            # Find the first <page> tag in the part\n",
    "            page_tag_match = re.search(r'<page:(\\d+)>', parts[i+1])\n",
    "            if page_tag_match:\n",
    "                # Derive the first <page> tag\n",
    "                first_page_number = int(page_tag_match.group(1)) - 1\n",
    "                first_page_tag = f'<page>{first_page_number}</page>\\n'\n",
    "                # Prepend the first <page> tag to the part\n",
    "                parts[i] = first_page_tag + parts[i]\n",
    "\n",
    "        # Reformat <page:123> to <page>123</page>\n",
    "        parts[i] = re.sub(r'<page:(\\d+)>', r'<page>\\1</page>', parts[i])\n",
    "        parts[i+1] = re.sub(r'<page:(\\d+)>', r'<page>\\1</page>', parts[i+1])\n",
    "\n",
    "        # Open the file using the variable\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            # Write the matched pattern and the following part to the file\n",
    "            file.write(parts[i] + parts[i+1])\n",
    "\n",
    "    # Handle hor_pattern separately\n",
    "    if hor_matches:\n",
    "        for match in hor_matches:\n",
    "            # Find the position of the hor_pattern match\n",
    "            match_pos = content.find(match)\n",
    "            # Find the previous <page> tag\n",
    "            prev_page_tag_pos = content.rfind('<page:', 0, match_pos)\n",
    "            next_session_pos = content.find('LEGISLATIVE COUNCIL.', match_pos)\n",
    "            if next_session_pos == -1:\n",
    "                next_session_pos = len(content)\n",
    "            # Extract the content from the previous <page> tag to the next session\n",
    "            extracted_content = content[prev_page_tag_pos:next_session_pos]\n",
    "            # Reformat <page:123> to <page>123</page> in the extracted content\n",
    "            extracted_content = re.sub(r'<page:(\\d+)>', r'<page>\\1</page>', extracted_content)\n",
    "            # Format the filename\n",
    "            hor_filename = f'1-{match}.txt'\n",
    "            # Concatenate output_dir_path with subdirectory and filename\n",
    "            hor_file_path = os.path.join(output_sessions_dir_path, hor_filename)\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(os.path.dirname(hor_file_path), exist_ok=True)\n",
    "            # Open the file using the variable\n",
    "            with open(hor_file_path, 'w', encoding='utf-8') as file:\n",
    "                # Write the extracted content to the file\n",
    "                file.write(extracted_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for this volume, to manage inconsistencies between the page numbers of the (edited) version of the Google pdf used for OCR and the Hathi PDF. This can be considered a bug. The best solution would be to re-transcribe (produce new .json source files) from the original Google Pdfs, although this would require additional post-OCR cleaning.\n",
    "\n",
    "def get_last_page_numbers(output_sessions_dir_path):\n",
    "    last_page_numbers = {}\n",
    "\n",
    "    # Sort filenames based on the numeric part at the beginning\n",
    "    filenames = sorted(os.listdir(output_sessions_dir_path), key=lambda x: int(re.findall(r'^\\d+', x)[0]) if re.findall(r'^\\d+', x) else float('inf'))\n",
    "\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(output_sessions_dir_path, filename)\n",
    "            with open(filepath, 'r') as file:\n",
    "                content = file.read()\n",
    "\n",
    "            # Find all <page> tags\n",
    "            page_tags = re.findall(r'<page>\\d+</page>', content)\n",
    "            if page_tags:\n",
    "                # Extract the last page number\n",
    "                last_page_number = int(re.findall(r'\\d+', page_tags[-1])[0])\n",
    "                last_page_numbers[filename] = last_page_number\n",
    "\n",
    "    return last_page_numbers\n",
    "\n",
    "def reset_page_tags(directory, last_page_numbers):\n",
    "    page_number = 1\n",
    "    previous_last_page_number = None\n",
    "    previous_filename = None\n",
    "\n",
    "    # Sort filenames based on the numeric part at the beginning\n",
    "    filenames = sorted(os.listdir(directory), key=lambda x: int(re.findall(r'^\\d+', x)[0]) if re.findall(r'^\\d+', x) else float('inf'))\n",
    "\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r') as file:\n",
    "                content = file.read()\n",
    "\n",
    "            # Find all <page> tags\n",
    "            page_tags = re.findall(r'<page>\\d+</page>', content)\n",
    "\n",
    "            if page_tags:\n",
    "                first_page_number = int(re.findall(r'\\d+', page_tags[0])[0])\n",
    "\n",
    "                if previous_last_page_number is not None and first_page_number == previous_last_page_number:\n",
    "                    # Retrieve the updated last page number from the previous file\n",
    "                    if previous_filename:\n",
    "                        previous_filepath = os.path.join(directory, previous_filename)\n",
    "                        with open(previous_filepath, 'r') as prev_file:\n",
    "                            prev_content = prev_file.read()\n",
    "                        prev_page_tags = re.findall(r'<page>\\d+</page>', prev_content)\n",
    "                        if prev_page_tags:\n",
    "                            new_last_page_number = int(re.findall(r'\\d+', prev_page_tags[-1])[0])\n",
    "                            # Start page numbering using new_last_page_number\n",
    "                            page_number = new_last_page_number\n",
    "\n",
    "            # Replace each <page> tag with the new sequence number\n",
    "            for tag in page_tags:\n",
    "                new_tag = f'<page>{page_number}</page>'\n",
    "                content = content.replace(tag, new_tag, 1)\n",
    "                page_number += 1\n",
    "\n",
    "            # Write the modified content back to the file\n",
    "            with open(filepath, 'w') as file:\n",
    "                file.write(content)\n",
    "\n",
    "            # Update the previous last page number and filename\n",
    "            if page_tags:\n",
    "                previous_last_page_number = int(re.findall(r'\\d+', page_tags[-1])[0])\n",
    "                previous_filename = filename\n",
    "\n",
    "# First pass to get the last page numbers\n",
    "last_page_numbers = get_last_page_numbers(output_sessions_dir_path)\n",
    "\n",
    "# Second pass to reset the page tags\n",
    "reset_page_tags(output_sessions_dir_path, last_page_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../output/sessions/3.August29-September25-1901/1-Thursday, 29th August, 1901.txt\n",
      "Finished processing file: ../output/sessions/3.August29-September25-1901/1-Thursday, 29th August, 1901.txt\n",
      "Processing file: ../output/sessions/3.August29-September25-1901/10-Wednesday, 18th September, 1901.txt\n",
      "Finished processing file: ../output/sessions/3.August29-September25-1901/10-Wednesday, 18th September, 1901.txt\n",
      "Processing file: ../output/sessions/3.August29-September25-1901/11-Friday, 20th September, 1901.txt\n",
      "Finished processing file: ../output/sessions/3.August29-September25-1901/11-Friday, 20th September, 1901.txt\n",
      "Processing file: ../output/sessions/3.August29-September25-1901/12-Tuesday, 24th September, 1901.txt\n",
      "Finished processing file: ../output/sessions/3.August29-September25-1901/12-Tuesday, 24th September, 1901.txt\n",
      "Processing file: ../output/sessions/3.August29-September25-1901/13-Wednesday, 25th September, 1901.txt\n",
      "Finished processing file: ../output/sessions/3.August29-September25-1901/13-Wednesday, 25th September, 1901.txt\n",
      "Processing file: ../output/sessions/3.August29-September25-1901/2-Wednesday, 4th September, 1901.txt\n",
      "Finished processing file: ../output/sessions/3.August29-September25-1901/2-Wednesday, 4th September, 1901.txt\n",
      "Processing file: ../output/sessions/3.August29-September25-1901/3-Thursday, 5th September, 1901.txt\n",
      "Finished processing file: ../output/sessions/3.August29-September25-1901/3-Thursday, 5th September, 1901.txt\n",
      "Processing file: ../output/sessions/3.August29-September25-1901/4-Friday, 6th September, 1901.txt\n",
      "Finished processing file: ../output/sessions/3.August29-September25-1901/4-Friday, 6th September, 1901.txt\n",
      "Processing file: ../output/sessions/3.August29-September25-1901/5-Tuesday, 10th September, 1901.txt\n",
      "Finished processing file: ../output/sessions/3.August29-September25-1901/5-Tuesday, 10th September, 1901.txt\n",
      "Processing file: ../output/sessions/3.August29-September25-1901/6-Wednesday, 11th September, 1901.txt\n",
      "Finished processing file: ../output/sessions/3.August29-September25-1901/6-Wednesday, 11th September, 1901.txt\n",
      "Processing file: ../output/sessions/3.August29-September25-1901/7-Thursday, 12th September, 1901.txt\n",
      "Finished processing file: ../output/sessions/3.August29-September25-1901/7-Thursday, 12th September, 1901.txt\n",
      "Processing file: ../output/sessions/3.August29-September25-1901/8-Friday, 13th September, 1901.txt\n",
      "Finished processing file: ../output/sessions/3.August29-September25-1901/8-Friday, 13th September, 1901.txt\n",
      "Processing file: ../output/sessions/3.August29-September25-1901/9-Tuesday, 17th September, 1901.txt\n",
      "Finished processing file: ../output/sessions/3.August29-September25-1901/9-Tuesday, 17th September, 1901.txt\n"
     ]
    }
   ],
   "source": [
    "# Concatenate urls tot he Hathi Trust source documents, and add them below the page numbers.\n",
    " \n",
    "# Initialize the current Hathi page number\n",
    "current_hathi_page = hathi_starting_page\n",
    "\n",
    "def process_file(file_path, hathi_id, hathi_starting_page):\n",
    "    global current_hathi_page\n",
    "    \n",
    "    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "    # Read the content of the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()    \n",
    " \n",
    "    # Split the content by lines\n",
    "    lines = content.split('\\n')\n",
    "    new_lines = []\n",
    "    \n",
    "    # Iterate over each line and process <page> tags\n",
    "    for line in lines:\n",
    "        page_match = re.match(r'<page>(\\d+)</page>', line)\n",
    "        \n",
    "        if page_match:\n",
    "            # Keep the original page number\n",
    "            page_number = int(page_match.group(1))\n",
    "            new_lines.append(f'<page>{page_number}</page>')\n",
    "            \n",
    "            # Calculate the HathiTrust page number based on the delta\n",
    "            hathi_page_number = hathi_starting_page + (page_number - 1)\n",
    "            url = f'https://babel.hathitrust.org/cgi/pt?id={hathi_id}&seq={hathi_page_number}'\n",
    "            new_lines.append(f'<url>{url}</url>')\n",
    "            print(f\"Updated line: <page>{page_number}</page> and <url>{url}</url>\")\n",
    "        else:\n",
    "            url_match = re.match(r'<url>https://babel.hathitrust.org/cgi/pt\\?id=.*&seq=\\d+</url>', line)\n",
    "            if url_match:\n",
    "                # Skip the existing URL line\n",
    "                continue\n",
    "            else:\n",
    "                new_lines.append(line)\n",
    "    \n",
    "    # Join the new lines to form the updated content\n",
    "    updated_content = '\\n'.join(new_lines)\n",
    "    \n",
    "    # Write the updated content to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(updated_content)\n",
    "\n",
    "    print(f\"Finished processing file: {file_path}\")\n",
    "\n",
    "# Iterate over each file in the subdirectory\n",
    "for filename in sorted(os.listdir(output_sessions_dir_path)):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(output_sessions_dir_path, filename)\n",
    "        process_file(file_path, hathi_id, hathi_starting_page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
