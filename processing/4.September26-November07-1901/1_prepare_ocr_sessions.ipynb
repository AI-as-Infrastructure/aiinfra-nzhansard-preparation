{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs the primary preparation of OCR content generated in Microsoft AI Studio and manually added to the repo in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json, re, logging\n",
    "\n",
    "# Manually set the root directory path\n",
    "root_dir = '../../'\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "from exclusions import excluded_strings, excluded_patterns\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# If the logger has handlers, remove them\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "# Create a new file handler that overwrites the log file each time\n",
    "file_handler = logging.FileHandler('exclusions.log', mode='w')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create a logging format\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handler to the logger\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Log the excluded strings and patterns\n",
    "logger.info(f\"Excluded strings: {excluded_strings}, Excluded patterns: {excluded_patterns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the OCR content (manually produced in and downloaded from Microsoft AI Studio) and associated positional data to generate page files from the two column layout, removing digitization metadata.\n",
    "# Needs to be performed manually for each session, defining the source file name here and running all remaining cells before repeating.\n",
    "\n",
    "# Define the primary source file you're working with\n",
    "hansard_source_file_name = '4.September26-November07-1901'  # replace with your file name\n",
    "\n",
    "# Concatenate hansard_source_file to source_dir and output_dir_path\n",
    "source_dir_path = os.path.join('../../source', hansard_source_file_name)\n",
    "output_pages_dir_path = os.path.join('../../output', 'pages', hansard_source_file_name)\n",
    "output_sessions_dir_path = os.path.join('../../output', 'sessions', hansard_source_file_name)\n",
    "\n",
    "# Check if source directory exists\n",
    "if not os.path.exists(source_dir_path):\n",
    "    os.makedirs(source_dir_path)\n",
    "\n",
    "# Check if output directory exists\n",
    "if not os.path.exists(output_pages_dir_path):\n",
    "    os.makedirs(output_pages_dir_path)\n",
    "\n",
    "\n",
    "# Get all files in the source directory\n",
    "files_in_directory = os.listdir(source_dir_path)\n",
    "\n",
    "# Filter for json files\n",
    "json_files = [file for file in files_in_directory if file.endswith('.json')]\n",
    "\n",
    "# Now you can iterate over the json_files list and process each file\n",
    "for json_file in json_files:\n",
    "    input_source_file = os.path.join(source_dir_path, json_file)\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open(input_source_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Access the \"analyzeResult\"\n",
    "    analyze_result = data['analyzeResult']\n",
    "\n",
    "    # Iterate over each page\n",
    "    for page in analyze_result['pages']:\n",
    "        # Create a variable to store the page content\n",
    "        page_content = \"\"\n",
    "\n",
    "        # Calculate the x-coordinate that separates the two columns\n",
    "        column_separator = page['width'] / 2\n",
    "\n",
    "        # Divide the text lines into two groups based on their x-coordinate\n",
    "        left_column_lines = [line for line in page['lines'] if min(line['polygon'][::2]) < column_separator]\n",
    "        right_column_lines = [line for line in page['lines'] if min(line['polygon'][::2]) >= column_separator]\n",
    "\n",
    "        # Sort each group of text lines by their y-coordinate (top to bottom)\n",
    "        left_column_lines.sort(key=lambda line: min(line['polygon'][1::2]))\n",
    "        right_column_lines.sort(key=lambda line: min(line['polygon'][1::2]))\n",
    "\n",
    "        # Combine the sorted text lines from the two columns\n",
    "        sorted_lines = left_column_lines + right_column_lines\n",
    "\n",
    "\n",
    "        # Define the areas of the page you want to exclude\n",
    "        x_range = (0, 0.1 * page['width'])  # Capture the left ~10% of the page. Callibrate to suit.\n",
    "        y_range = (0.95 * page['height'], page['height'])  # Capture the bottom ~10% of the page. Callibrate to suit.\n",
    "        header_y_range = (0, 0.1 * page['height'])  # Capture the top ~10% of the page. Callibrate to suit.\n",
    "\n",
    "        # Iterate over each text line in the page\n",
    "        for line in sorted_lines:\n",
    "            # Check if any of the y-coordinates of the line fall within the y_range\n",
    "            y_coordinates = line['polygon'][1::2]\n",
    "            if any(y_range[0] <= y <= y_range[1] for y in y_coordinates) or any(header_y_range[0] <= y <= header_y_range[1] for y in y_coordinates):\n",
    "                # If y-coordinate condition is met, log the line and continue to next iteration\n",
    "                logging.info(f\"Excluded due to y_range or header_y_range: {line}\")\n",
    "                continue\n",
    "\n",
    "            # If condition is false, add the line to the page content\n",
    "            content = line['content']\n",
    "            page_content += content + \" \"\n",
    "\n",
    "        # Write the page content to a new file in the output directory\n",
    "        output_file_path = os.path.join(output_pages_dir_path, f'page_{page[\"pageNumber\"]}.md')\n",
    "        with open(output_file_path, 'w') as f:\n",
    "            f.write(page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the pages into a single master content file, for cleaning.\n",
    "\n",
    "combined_file = os.path.join(output_pages_dir_path, 'all_pages_master.txt')\n",
    "\n",
    "def sort_key(filename):\n",
    "    match = re.search(r'page_(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return float('inf')\n",
    "\n",
    "with open(combined_file, 'w') as outfile:\n",
    "    for filename in sorted(os.listdir(output_pages_dir_path), key=sort_key):\n",
    "        if filename.startswith('page_') and filename.endswith('.md'):\n",
    "            page_number = filename.replace('page_', '').replace('.md', '')\n",
    "            outfile.write(f'<page:{page_number}>\\n')\n",
    "            with open(os.path.join(output_pages_dir_path, filename), 'r') as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line.lstrip())  # strip leading whitespace\n",
    "                outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the master content file, using local processing\n",
    "\n",
    "# Read the content\n",
    "with open(combined_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Remove patterns matching excluded patterns\n",
    "for pattern in excluded_patterns:\n",
    "    lines = [re.sub(pattern, '', line) for line in lines]\n",
    "\n",
    "# Remove strings matching excluded strings\n",
    "for string in excluded_strings:\n",
    "    logging.info(f\"Removing string: {string}\")\n",
    "    lines = [line.replace(string, '') for line in lines]\n",
    "\n",
    "# Remove unwanted indentation\n",
    "lines = [line.lstrip() for line in lines]\n",
    "\n",
    "# Write the cleaned content back\n",
    "master_file_path = os.path.join(output_pages_dir_path, 'all_pages_master.txt')\n",
    "with open(master_file_path, 'w') as file:\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the cleaned master file into sessions, adding page tags.\n",
    "\n",
    "# Open the master file\n",
    "with open(master_file_path, 'r') as file:\n",
    "    # Read the content\n",
    "    content = file.read()\n",
    "\n",
    "    # Define the pattern for 'LEGISLATIVE COUNCIL.' followed by a date\n",
    "    pattern = r'(LEGISLATIVE COUNCIL\\.\\s*(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday), (?:\\d{1,2}|[IVX]{1,3})(?:st|nd|rd|th) \\w+, \\d{4})'\n",
    "\n",
    "    # Find all matches of the pattern\n",
    "    matches = re.findall(pattern, content)\n",
    "\n",
    "    # Split the content based on the pattern\n",
    "    parts = re.split(pattern, content)\n",
    "\n",
    "    # Iterate over the parts, skipping the first\n",
    "    for i in range(1, len(parts), 2):\n",
    "        # Extract the date string following 'LEGISLATIVE COUNCIL.'\n",
    "        date_match = re.search(r'(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday), (?:\\d{1,2}|[IVX]{1,3})(?:st|nd|rd|th) \\w+, \\d{4}', parts[i])\n",
    "        date = date_match.group() if date_match else 'No date'\n",
    "\n",
    "        # Format the filename\n",
    "        filename = f'{i//2+1}-{date}.txt'\n",
    "\n",
    "        # Concatenate output_dir_path with subdirectory and filename\n",
    "        file_path = os.path.join(output_sessions_dir_path, filename)\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "        # Check if the part starts with a <page> tag\n",
    "        if not parts[i].strip().startswith('<page:'):\n",
    "            # Find the first <page> tag in the part\n",
    "            page_tag_match = re.search(r'<page:(\\d+)>', parts[i+1])\n",
    "            if page_tag_match:\n",
    "                # Derive the first <page> tag\n",
    "                first_page_number = int(page_tag_match.group(1)) - 1\n",
    "                first_page_tag = f'<page:{first_page_number}>\\n'\n",
    "                # Prepend the first <page> tag to the part\n",
    "                parts[i] = first_page_tag + parts[i]\n",
    "\n",
    "        # Open the file using the variable\n",
    "        with open(file_path, 'w') as file:\n",
    "            # Write the matched pattern and the following part to the file\n",
    "            file.write(parts[i] + parts[i+1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
