{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat <page> tags to match AU and UK.\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define session variable\n",
    "session = '1.July01-July26-1901'\n",
    "\n",
    "# Define subdirectory path\n",
    "subdirectory_path = 'txt/hofreps'\n",
    "\n",
    "# Construct the directory path dynamically\n",
    "directory = os.path.join(subdirectory_path, session)\n",
    "\n",
    "def replace_text_in_files(directory):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            # Replace <page:123> with <page>123</page>\n",
    "            new_content = re.sub(r'<page:(\\d+)>', r'<page>\\1</page>', content)\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(new_content)\n",
    "\n",
    "# Call the function with the dynamically constructed directory\n",
    "replace_text_in_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation, for cell below.\n",
    "\n",
    "session = '1.July01-July26-1901' \n",
    "hathi_id = 'uc1.32106019788238'\n",
    "starting_page = 29\n",
    "session = '2.July30-August28-1901' \n",
    "hathi_id = 'uc1.32106019788246'\n",
    "starting_page = 31\n",
    "session = '3.August29-September25-1901'\n",
    "hathi_id = 'uc1.32106019788253'\n",
    "starting_page = 21\n",
    "session = '4.September26-November07-1901'\n",
    "hathi_id = 'uc1.32106019788261'\n",
    "starting_page = 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct page numbers so they refer to the Hansard source rather than the Google pdf, and add the corresponding URL for the Hathi online version.\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define the folder and starting_page\n",
    "session = '3.August29-September25-1901'\n",
    "hathi_id = 'uc1.32106019788253'\n",
    "starting_page = 5\n",
    "delete_unwanted = 'No'  # Option to delete unwanted content before a certain page\n",
    "\n",
    "# Construct the path to the subdirectory\n",
    "subdirectory_path = os.path.join('txt/hofreps', session)\n",
    "\n",
    "# Initialize the global page number\n",
    "global_page_number = 1\n",
    "unwanted_deleted = False\n",
    "\n",
    "def process_file(file_path, hathi_id, starting_page, delete_unwanted):\n",
    "    global global_page_number\n",
    "    global unwanted_deleted\n",
    "\n",
    "    # Read the content of the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # If delete_unwanted is set to 'Yes' and unwanted content has not been deleted yet\n",
    "    if delete_unwanted == 'Yes' and not unwanted_deleted:\n",
    "        start_index = content.find(f'<page>{starting_page}</page>')\n",
    "        if start_index != -1:\n",
    "            content = content[start_index:]\n",
    "            unwanted_deleted = True\n",
    "    \n",
    "    # Find all <page> tags and their corresponding numbers\n",
    "    page_tags = re.findall(r'<page>(\\d+)</page>', content)\n",
    "    print(f\"Found page tags: {page_tags}\")\n",
    "    \n",
    "    # Split the content by lines\n",
    "    lines = content.split('\\n')\n",
    "    new_lines = []\n",
    "    url_added = False\n",
    "    \n",
    "    # Iterate over each line and process <page> tags\n",
    "    for line in lines:\n",
    "        page_match = re.match(r'<page>(\\d+)</page>', line)\n",
    "        \n",
    "        if page_match:\n",
    "            page_number = int(page_match.group(1))\n",
    "            adjusted_page_number = global_page_number\n",
    "            global_page_number += 1\n",
    "            print(f\"Original page number: {page_number}, Adjusted page number: {adjusted_page_number}\")\n",
    "            new_lines.append(f'<page>{adjusted_page_number}</page>')\n",
    "            url = f'<url>https://babel.hathitrust.org/cgi/pt?id={hathi_id}&seq={adjusted_page_number}</url>'\n",
    "            new_lines.append(url)\n",
    "            url_added = True\n",
    "        else:\n",
    "            if re.match(r'<url>https://babel.hathitrust.org/cgi/pt\\?id=.*&seq=\\d+</url>', line):\n",
    "                new_lines.append(line)\n",
    "                url_added = True\n",
    "            else:\n",
    "                new_lines.append(line)\n",
    "                url_added = False\n",
    "    \n",
    "    # Join the new lines to form the updated content\n",
    "    updated_content = '\\n'.join(new_lines)\n",
    "    \n",
    "    # Write the updated content back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(updated_content)\n",
    "\n",
    "# Iterate over each file in the subdirectory\n",
    "for filename in os.listdir(subdirectory_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(subdirectory_path, filename)\n",
    "        process_file(file_path, hathi_id, starting_page, delete_unwanted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
